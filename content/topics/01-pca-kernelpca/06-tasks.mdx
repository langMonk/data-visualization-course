## Exercises

### 1. Perform PCA for Breast Cancer Dataset (3 points)

* You can find this dataset in the scikit-learn library, import it and convert to pandas dataframe. Original labels are '0' and '1'; for better readability change these names to: 'benign' and 'malignant'.

```python
# TODO: Import the breast cancer dataset and create dataframe
```

* Visualize correlations between pairs of features (due to the greater number of features use pandas corr() function instead of pairplot and seaborn heatmap()).

```python
# TODO: Visualize feature correlations
```

* Perform PCA and visualize the data.

```python
# TODO: Apply PCA and visualize the results
```

* Examine explained variance, draw a plot showing relation between total explained variance and number of principal components used.

```python
# TODO: Plot explained variance
```

* Use recursive feature elimination (available in scikit-learn module) or another feature ranking algorithm to split 30 features into 15 "more important" and 15 "less important" features. Then repeat the last step from the full data set - draw a plot showing relation between total explained variance and number of principal components used for all 3 cases. Explain the result briefly.

```python
# TODO: Apply feature selection and compare PCA results
```

### 2: KernelPCA (3 points)

* Visualize in 2D datasets used in this lab, experiment with the parameters of the KernelPCA method change kernel and gamma params. Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html

```python
# TODO: Apply KernelPCA with different parameters
```

### 3: Classic PCA Analysis on MNIST (4 points)

* Download the MNIST data set (there is a function to load this set in libraries such as scikit-learn, keras). It is a collection of black and white photos of handwritten digits with a resolution of 28x28 pixels, which together gives 784 dimensions.

* Try to visualize this dataset using PCA and KernelPCA, don't expect full separation of the data.

* Similar to the exercises, examine explained variance. Draw explained variance vs number of principal Components plot.

* Find number of principal components for 99%, 95%, 90%, and 85% of explained variance.

* Draw some sample MNIST digits and from PCA of its images transform data back to its original space (https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA.inverse_transform). Make an inverse transformation for number of components corresponding with explained variance shown above and draw the reconstructed images. The idea of this exercise is to see visually how depending on the number of components some information is lost.

* Perform the same reconstruction using KernelPCA (make comparisons for the same components number): https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html#sklearn.decomposition.KernelPCA.inverse_transform


### (*) Optional: 4. Modern PCA Applications (2 points)

Choose ONE of the following advanced tasks:

#### Option A: Robust PCA for Noisy Image Reconstruction

1. Take 10 sample images from MNIST
2. Add random noise and some structured outliers (e.g., salt and pepper noise)
3. Apply both standard PCA and Robust PCA for denoising
4. Compare the results quantitatively (using metrics like PSNR, SSIM) and qualitatively

#### Option B: Incremental PCA for Large Datasets

1. Load the Fashion-MNIST dataset (a more complex alternative to MNIST)
2. Implement a streaming data scenario by processing the dataset in small batches
3. Apply Incremental PCA and compare its performance (both in terms of computation time and accuracy) with standard PCA
4. Visualize how the principal components evolve as more data is processed

#### Option C: PCA in Convolutional Autoencoder

1. Implement a simple convolutional autoencoder for MNIST digit reconstruction
2. Extract the bottleneck layer representations (latent space)
3. Compare these representations with traditional PCA by:
- Visualizing the 2D projections of both
- Measuring reconstruction quality at comparable compression ratios
- Analyzing the feature importance in both approaches



### Important:

For excersise 3 and 4 write a brief analysis (200-500 words) addressing:

1. What are the key strengths and limitations of the PCA variant you explored?
2. In what real-world scenarios would this approach be particularly useful?
3. How does the computational complexity scale with dataset size?
4. What improvements or modifications would you suggest to the method?