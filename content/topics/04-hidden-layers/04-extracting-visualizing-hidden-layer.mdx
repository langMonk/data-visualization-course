## Extracting Hidden Layer Activations

```python
# Keras Model has the summary function that prints data about model architecture
model.summary()

# We are interested in downloading the activation of hidden layers, because the dropout layers are between them,
# we need to properly select the index of the three dense layers.
get_layer_output = K.function([model.layers[0].input, model.layers[0].input, model.layers[0].input],
                              [model.layers[0].output, model.layers[2].output, model.layers[4].output])

layer1_output, layer2_output, layer3_output = get_layer_output([X_train])

train_ids = [np.arange(len(Y_train))[Y_train[:,i] == 1] for i in range(10)]
```

## Visualizing Network Activations

The 2 visualizations below show how neuron activations work:

1. First animation shows what an example digit (number 5) looks like and what activations of neurons look like in hidden layers of the neural network.

2. Second animation checks the similarity in behavior for frames showing the same digit by looking at the ensemble properties. In this case, ensemble properties refers to how the neurons behave on average for a large number of frames showing the same digit.

After summing up the responses of as few as 20-30 frames, the pattern in the second hidden layer is almost static. After combining about 70-80 frames, the pattern in the first hidden layer also appears static. This supports the idea that only a subset of all neurons is involved in the recognition of individual digits.

This observation is important when we think about using neural networks for data visualization. We can clearly see that the activations generated by examples belonging to the same class are less chaotic than the examples themselves; therefore, their visualization should give a more clustered structure.