## Neural Network for Representation Learning

In this section, we'll train a neural network on our spatial transcriptomics data. Instead of focusing solely on classification accuracy, we're interested in the network's ability to learn meaningful representations of the data in its hidden layers.

```python
# Split data into training and test sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Get number of classes
nb_classes = Y.shape[1]

# Define dropout rate
dropout = 0.3

# Build neural network with 2 hidden layers
model = Sequential()
model.add(Dense(256, activation='relu', input_shape=(X_train.shape[1],)))
model.add(Dropout(dropout))
model.add(Dense(64, activation='relu'))
model.add(Dropout(dropout))
model.add(Dense(nb_classes, activation='softmax'))

# Compile the model
model.compile(loss='categorical_crossentropy',
              optimizer=Adam(),
              metrics=['accuracy'])

# Print model summary
model.summary()

# Train the neural network
history = model.fit(X_train, Y_train,
                    batch_size=64,
                    epochs=30,
                    verbose=1,
                    validation_split=0.2)

# Plot training history
plt.figure(figsize=(15, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper right')

plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='lower right')
plt.tight_layout()
plt.show()

# Evaluate on test set
loss, accuracy = model.evaluate(X_test, Y_test, verbose=0)
print(f"Test Loss: {loss:.4f}")
print(f"Test Accuracy: {accuracy:.4f}")

# Extract hidden layer activations
get_layer_output = K.function([model.layers[0].input],
                             [model.layers[0].output, model.layers[2].output])

# Get activations for training and test data
layer1_output, layer2_output = get_layer_output([X_train])
test_layer1_output, test_layer2_output = get_layer_output([X_test])

print(f"Raw data shape: {X_train.shape}")
print(f"Layer 1 output shape: {layer1_output.shape}")
print(f"Layer 2 output shape: {layer2_output.shape}")
```

Now that we have trained our neural network, we can use the activations of the hidden layers as alternative representations of our data. These activations may capture more abstract and biologically meaningful patterns than the raw gene expression data.
